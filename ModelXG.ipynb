{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load all CSV files from the \"Preprocessed Data\" folder\n",
    "folder_path = r'Preprocessed_Data\\node22c Oct23_Jan24'\n",
    "dataframes = []\n",
    "\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            dataframes.append(df)\n",
    "\n",
    "# Combine all dataframes into a single dataframe\n",
    "combined_data = pd.concat(dataframes, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date Time</th>\n",
       "      <th>Slave_Device1_CH1_FLOW m3/h</th>\n",
       "      <th>Slave_Device1_CH2_VELOCITY m/s</th>\n",
       "      <th>Slave_Device1_CH3_Pressure BAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1130455</th>\n",
       "      <td>2023-10-30 22:04:06</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130456</th>\n",
       "      <td>2023-10-30 22:04:09</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130457</th>\n",
       "      <td>2023-10-30 22:04:13</td>\n",
       "      <td>74.5</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130458</th>\n",
       "      <td>2023-10-30 22:04:17</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130459</th>\n",
       "      <td>2023-10-30 22:45:41</td>\n",
       "      <td>68.5</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Date Time  Slave_Device1_CH1_FLOW m3/h  \\\n",
       "1130455  2023-10-30 22:04:06                         74.0   \n",
       "1130456  2023-10-30 22:04:09                         75.0   \n",
       "1130457  2023-10-30 22:04:13                         74.5   \n",
       "1130458  2023-10-30 22:04:17                         74.0   \n",
       "1130459  2023-10-30 22:45:41                         68.5   \n",
       "\n",
       "         Slave_Device1_CH2_VELOCITY m/s  Slave_Device1_CH3_Pressure BAR  \n",
       "1130455                            0.50                            1.25  \n",
       "1130456                            0.50                            1.25  \n",
       "1130457                            0.50                            1.25  \n",
       "1130458                            0.50                            1.24  \n",
       "1130459                            0.46                            1.26  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Slave_Device1_CH1_FLOW m3/h</th>\n",
       "      <th>Slave_Device1_CH2_VELOCITY m/s</th>\n",
       "      <th>Slave_Device1_CH3_Pressure BAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.130460e+06</td>\n",
       "      <td>1.130460e+06</td>\n",
       "      <td>1.130460e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.966216e+01</td>\n",
       "      <td>4.002126e-01</td>\n",
       "      <td>1.146587e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.946315e+01</td>\n",
       "      <td>1.978137e-01</td>\n",
       "      <td>3.545427e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.700000e+01</td>\n",
       "      <td>3.200000e-01</td>\n",
       "      <td>1.180000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.225000e+01</td>\n",
       "      <td>4.200000e-01</td>\n",
       "      <td>1.260000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.450000e+01</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>1.340000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.490000e+02</td>\n",
       "      <td>2.510000e+00</td>\n",
       "      <td>2.510000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Slave_Device1_CH1_FLOW m3/h  Slave_Device1_CH2_VELOCITY m/s  \\\n",
       "count                 1.130460e+06                    1.130460e+06   \n",
       "mean                  5.966216e+01                    4.002126e-01   \n",
       "std                   2.946315e+01                    1.978137e-01   \n",
       "min                   0.000000e+00                    0.000000e+00   \n",
       "25%                   4.700000e+01                    3.200000e-01   \n",
       "50%                   6.225000e+01                    4.200000e-01   \n",
       "75%                   7.450000e+01                    5.000000e-01   \n",
       "max                   2.490000e+02                    2.510000e+00   \n",
       "\n",
       "       Slave_Device1_CH3_Pressure BAR  \n",
       "count                    1.130460e+06  \n",
       "mean                     1.146587e+00  \n",
       "std                      3.545427e-01  \n",
       "min                      0.000000e+00  \n",
       "25%                      1.180000e+00  \n",
       "50%                      1.260000e+00  \n",
       "75%                      1.340000e+00  \n",
       "max                      2.510000e+00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date Time' to datetime and extract time-based features\n",
    "combined_data['Date Time'] = pd.to_datetime(combined_data['Date Time'])\n",
    "combined_data['hour'] = combined_data['Date Time'].dt.hour\n",
    "combined_data['day_of_week'] = combined_data['Date Time'].dt.dayofweek\n",
    "\n",
    "# Rename columns for easier access\n",
    "combined_data.rename(columns={\n",
    "    'Slave_Device1_CH1_FLOW m3/h': 'flow',\n",
    "    'Slave_Device1_CH3_Pressure BAR': 'pressure'\n",
    "}, inplace=True)\n",
    "\n",
    "# Create lag features for flow and pressure\n",
    "combined_data['flow_lag1'] = combined_data['flow'].shift(1)\n",
    "combined_data['pressure_lag1'] = combined_data['pressure'].shift(1)\n",
    "\n",
    "# Calculate rolling mean for flow and pressure\n",
    "combined_data['flow_roll_mean'] = combined_data['flow'].rolling(window=5).mean()\n",
    "combined_data['pressure_roll_mean'] = combined_data['pressure'].rolling(window=5).mean()\n",
    "\n",
    "# Drop rows with NaN values after creating lag and rolling features\n",
    "combined_data.dropna(inplace=True)\n",
    "\n",
    "# Define features and target variable\n",
    "features = combined_data[['flow', 'pressure', 'hour', 'day_of_week', \n",
    "                          'flow_lag1', 'pressure_lag1', \n",
    "                          'flow_roll_mean', 'pressure_roll_mean']]\n",
    "\n",
    "# labels = combined_data['burst_event']  # Assuming a 'burst_event' column for binary target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date Time', 'flow', 'Slave_Device1_CH2_VELOCITY m/s', 'pressure',\n",
       "       'hour', 'day_of_week', 'flow_lag1', 'pressure_lag1', 'flow_roll_mean',\n",
       "       'pressure_roll_mean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define thresholds\n",
    "FLOW_THRESHOLD = 100  # Flow rate threshold in m³/s\n",
    "PRESSURE_THRESHOLD = 3  # Pressure threshold in bar\n",
    "FLOW_CHANGE_THRESHOLD = 10  # Threshold for a sudden increase in flow rate\n",
    "PRESSURE_CHANGE_THRESHOLD = 0.5  # Threshold for a sudden increase in pressure\n",
    "\n",
    "# Calculate change rates\n",
    "combined_data['flow_change'] = combined_data['flow'].diff().abs()\n",
    "combined_data['pressure_change'] = combined_data['pressure'].diff().abs()\n",
    "\n",
    "# Define burst event based on threshold conditions\n",
    "combined_data['burst_event'] = np.where(\n",
    "    (combined_data['flow'] >= FLOW_THRESHOLD) |\n",
    "    (combined_data['pressure'] >= PRESSURE_THRESHOLD) |\n",
    "    (combined_data['flow_change'] >= FLOW_CHANGE_THRESHOLD) |\n",
    "    (combined_data['pressure_change'] >= PRESSURE_CHANGE_THRESHOLD),\n",
    "    1,  # Label as burst\n",
    "    0   # No burst\n",
    ")\n",
    "\n",
    "# Drop rows with any NaN values in the dataset after feature engineering\n",
    "combined_data.dropna(inplace=True)\n",
    "\n",
    "# Now, you should have consistent row counts across all columns, allowing you to define features and labels\n",
    "features = combined_data[['flow', 'pressure', 'hour', 'day_of_week', \n",
    "                          'flow_lag1', 'pressure_lag1', \n",
    "                          'flow_roll_mean', 'pressure_roll_mean']]\n",
    "labels = combined_data['burst_event']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.130455e+06\n",
       "mean     7.482032e-02\n",
       "std      2.631013e-01\n",
       "min      0.000000e+00\n",
       "25%      0.000000e+00\n",
       "50%      0.000000e+00\n",
       "75%      0.000000e+00\n",
       "max      1.000000e+00\n",
       "Name: burst_event, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = combined_data['burst_event']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lightgbm\\engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 67698, number of negative: 836666\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.035606 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1365\n",
      "[LightGBM] [Info] Number of data points in the train set: 904364, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074857 -> initscore=-2.514368\n",
      "[LightGBM] [Info] Start training from score -2.514368\n",
      "[[209171     37]\n",
      " [   126  16757]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    209208\n",
      "           1       1.00      0.99      1.00     16883\n",
      "\n",
      "    accuracy                           1.00    226091\n",
      "   macro avg       1.00      1.00      1.00    226091\n",
      "weighted avg       1.00      1.00      1.00    226091\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Define LightGBM dataset format\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a LightGBM dataset\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# Set parameters (tuned for faster training)\n",
    "params = {\n",
    "    'objective': 'binary',          # for binary classification\n",
    "    'boosting_type': 'gbdt',        # traditional gradient boosting\n",
    "    'n_estimators': 100,            # number of boosting rounds\n",
    "    'max_depth': 10,                # limit depth to reduce complexity\n",
    "    'num_leaves': 31,               # default for LightGBM, balance with max_depth\n",
    "    'learning_rate': 0.1,           # faster training\n",
    "    'n_jobs': -1                    # use all available cores\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "model = lgb.train(params, train_data, valid_sets=[test_data])\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = [1 if p > 0.5 else 0 for p in y_pred]  # convert probabilities to binary predictions\n",
    "print(confusion_matrix(y_test, y_pred_binary))\n",
    "print(classification_report(y_test, y_pred_binary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # Standardize features\n",
    "# scaler = StandardScaler()\n",
    "# features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# # Split the dataset\n",
    "# X_train, X_test, y_train, y_test = train_test_split(features_scaled, labels, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# # Initialize and train the model with hyperparameter tuning\n",
    "# model = RandomForestClassifier()\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200],\n",
    "#     'max_depth': [None, 10, 20],\n",
    "# }\n",
    "# grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Model evaluation\n",
    "# y_pred = grid_search.predict(X_test)\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Example parameters\n",
    "# TIME_STEPS = 30  # Number of time steps in each sequence\n",
    "TIME_STEPS = 20\n",
    "FEATURES = 3  # Number of features: FLOW, VELOCITY, PRESSURE\n",
    "\n",
    "# Assuming `data` is your concatenated DataFrame with normalized values\n",
    "\n",
    "# Prepare sequences\n",
    "def create_sequences(data, time_steps=TIME_STEPS):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        seq = data.iloc[i:i + time_steps].values\n",
    "        label = data.iloc[i + time_steps]['burst_event']  # Assuming 'Burst' is the target column\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "combined_data[['flow', 'Slave_Device1_CH2_VELOCITY m/s', 'pressure']] = scaler.fit_transform(combined_data[['flow', 'Slave_Device1_CH2_VELOCITY m/s', 'pressure']])\n",
    "\n",
    "# Split data into sequences\n",
    "X, y = create_sequences(combined_data)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# LSTM Model\n",
    "# Reduced time steps and batch size\n",
    "BATCH_SIZE = 64  # Increase batch size\n",
    "\n",
    "# Simplified LSTM Model\n",
    "model = Sequential([\n",
    "    LSTM(25, input_shape=(TIME_STEPS, FEATURES)),  # Reduce units and remove return_sequences\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train with reduced epochs for testing\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=BATCH_SIZE, validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on new data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)  # Convert probabilities to binary predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
